{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# depending on how this notebook is invoked, sc may already be instantiated.\n",
    "# if we open ipython using the pyspark python kernel, then all the Spark stuff is already here and imported\n",
    "# but if we just open a plain ipython session, then we need to do all the imports and instantiation\n",
    "\n",
    "#from pyspark import SparkContext\n",
    "#sc = SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything starts from the \"SparkContext\". This object lives on the driver node and controls the whole application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x7f56b4c41990>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the SparkContext to create an RDD from some local data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myRDD = sc.parallelize(range(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:423"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the SparkContext to load in external data, from csv files or on a distributed filesystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "beer = sc.textFile(\"../data/beer.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[11] at textFile at NativeMethodAccessorImpl.java:-2"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RDD is distributed across the cluster, so if want to \"view\" any of it, we have to bring a piece back down to the driver node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'name calories sodium alcohol cost', u'Budweiser 144 15 4.7 0.43']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beer.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the result of sc.textFile() is an RDD where each element is a string, corredponding to each line of the file. We probably want to parse these lines and make something a little more useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'name', u'calories', u'sodium', u'alcohol', u'cost'],\n",
       " [u'Budweiser', u'144', u'15', u'4.7', u'0.43'],\n",
       " [u'Schlitz', u'151', u'19', u'4.9', u'0.43']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beer.map(lambda line: line.split(\" \")).take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get rid of that first line, since it represent the header info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "beer = beer.map(lambda line: line.split(\" \")).filter(lambda l:l[0]!=\"name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'Budweiser', u'144', u'15', u'4.7', u'0.43'],\n",
       " [u'Schlitz', u'151', u'19', u'4.9', u'0.43']]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beer.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've got an RDD of lists of strings, we probably want to parse this into something more useful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Beer = namedtuple(\"Beer\", (\"name\", \"calories\",\"sodium\",\"alcohol\",\"cost\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse(line):\n",
    "    n = line[0]\n",
    "    values = [float(val) for val in line[1:]]\n",
    "    return Beer(n, *values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "beer_parsed = beer.map(parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Beer(name=u'Budweiser', calories=144.0, sodium=15.0, alcohol=4.7, cost=0.43),\n",
       " Beer(name=u'Schlitz', calories=151.0, sodium=19.0, alcohol=4.9, cost=0.43)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beer_parsed.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Beer(name=u'Old_Milwaukee', calories=145.0, sodium=23.0, alcohol=4.6, cost=0.28),\n",
       " Beer(name=u'Pabst_Extra_Light', calories=68.0, sodium=15.0, alcohol=2.3, cost=0.38)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beer_parsed.filter(lambda a: a.cost<.4).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.5, 1),\n",
       " (0.4, 2),\n",
       " (0.77, 1),\n",
       " (0.44, 1),\n",
       " (0.48, 1),\n",
       " (0.76, 1),\n",
       " (0.42, 1),\n",
       " (0.38, 1),\n",
       " (0.28, 1),\n",
       " (0.46, 2),\n",
       " (0.43, 5),\n",
       " (0.47, 1),\n",
       " (0.79, 1),\n",
       " (0.73, 1)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beer_parsed.map(lambda a : (a.cost,1)).reduceByKey(lambda a,b: a+b).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLlib Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS, LogisticRegressionModel, LabeledPoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a classification example, where want to predict whether the beer cost less than or more then .5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepPoint(p):\n",
    "    expensive = 1 if p.cost >.5 else 0\n",
    "    return LabeledPoint(expensive,[p.calories, p.sodium, p.alcohol])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labeledPoints = beer_parsed.map(prepPoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(0.0, [144.0,15.0,4.7]),\n",
       " LabeledPoint(0.0, [151.0,19.0,4.9]),\n",
       " LabeledPoint(0.0, [157.0,15.0,0.9]),\n",
       " LabeledPoint(1.0, [170.0,7.0,5.2]),\n",
       " LabeledPoint(1.0, [152.0,11.0,5.0]),\n",
       " LabeledPoint(0.0, [145.0,23.0,4.6]),\n",
       " LabeledPoint(0.0, [175.0,24.0,5.5]),\n",
       " LabeledPoint(0.0, [149.0,27.0,4.7]),\n",
       " LabeledPoint(0.0, [99.0,10.0,4.3]),\n",
       " LabeledPoint(0.0, [113.0,8.0,3.7]),\n",
       " LabeledPoint(0.0, [140.0,18.0,4.6]),\n",
       " LabeledPoint(0.0, [102.0,15.0,4.1]),\n",
       " LabeledPoint(0.0, [135.0,11.0,4.2]),\n",
       " LabeledPoint(1.0, [150.0,19.0,4.7]),\n",
       " LabeledPoint(1.0, [149.0,6.0,5.0]),\n",
       " LabeledPoint(0.0, [68.0,15.0,2.3]),\n",
       " LabeledPoint(0.0, [139.0,19.0,4.4]),\n",
       " LabeledPoint(0.0, [144.0,24.0,4.9]),\n",
       " LabeledPoint(0.0, [72.0,6.0,2.9]),\n",
       " LabeledPoint(0.0, [97.0,7.0,4.2])]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeledPoints.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogisticRegressionWithLBFGS.train(labeledPoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error = 0.1\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model on training data\n",
    "labelsAndPredictions = labeledPoints.map(lambda p: (p.label, model.predict(p.features)))\n",
    "trainErr = labelsAndPredictions.filter(lambda (v, p): v != p).count() / float(labeledPoints.count())\n",
    "print(\"Training Error = \" + str(trainErr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very similar interface and style of thinking as we're used to with scikitlearn. But this Logistic Regression model is estimated on a whole RDD, which could easily be billions of data points. All the cleverness and innovation is in making model.train() work effectively in a distributed computing environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
