{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in _notmist.ipynb_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix (Nx784)\n",
    "- labels as float 1-hot encodings (Nx10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set-1H (200000, 784) (200000, 10)\n",
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set-1H (10000, 784) (10000, 10)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set-1H (10000, 784) (10000, 10)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 2 to [0.0, 1.0, 0.0 ...], 3 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "\n",
    "train_dataset_1h, train_labels_1h = reformat(train_dataset, train_labels)\n",
    "valid_dataset_1h, valid_labels_1h = reformat(valid_dataset, valid_labels)\n",
    "test_dataset_1h, test_labels_1h = reformat(test_dataset, test_labels)\n",
    "print('Training set-1H', train_dataset_1h.shape, train_labels_1h.shape)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set-1H', valid_dataset_1h.shape, valid_labels_1h.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set-1H', test_dataset_1h.shape, test_labels_1h.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression and Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# From Assignment 1\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "def LR_train(n_samples, C, train_ds, train_lbl, test_ds, test_lbl):\n",
    "\n",
    "    Xtrain = np.array(train_ds[0:n_samples,:,:])\n",
    "    Xtrain = np.reshape(Xtrain, (n_samples,784))\n",
    "    ytrain = np.reshape(train_lbl[0:n_samples], (n_samples,))\n",
    "    logreg = LogisticRegression(C=C,solver='sag', multi_class='ovr')\n",
    "    #gauss_wgts = np.random.normal(loc=0, scale=0.2, size=n_samples)\n",
    "    logreg.fit(Xtrain, ytrain)\n",
    "    \n",
    "    testX = np.reshape(test_ds, (len(test_ds), 784))\n",
    "    ypred = logreg.predict(testX)\n",
    "    \n",
    "    yresp = np.reshape(test_lbl, (len(test_lbl,)))\n",
    "    \n",
    "    return metrics.accuracy_score(yresp, ypred)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anna/anaconda2/lib/python2.7/site-packages/sklearn/linear_model/sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "print(LR_train(1000, .09, train_dataset, train_labels, test_dataset, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy_scores = []\n",
    "for c_val in np.arange(0.001, 1.0, .005):\n",
    "    #print(c_val)    \n",
    "    acc = LR_train(1000, c_val, train_dataset, train_labels, test_dataset, test_labels)\n",
    "    accuracy_scores.append(acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max accuracy 0.8599\n",
      "index 5\n",
      "[ 0.001  0.006  0.011  0.016  0.021  0.026  0.031  0.036]\n",
      "[0.82550000000000001, 0.85219999999999996, 0.85699999999999998, 0.85880000000000001, 0.85970000000000002, 0.8599, 0.85950000000000004, 0.85860000000000003]\n"
     ]
    }
   ],
   "source": [
    "print('max accuracy', max(accuracy_scores))\n",
    "print('index',accuracy_scores.index(max(accuracy_scores)))\n",
    "print(np.arange(.001,1.0,.005)[0:8])\n",
    "print(accuracy_scores[0:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accuracy_scores_1 = []\n",
    "for c_val in np.arange(0.021, .032, .0001):\n",
    "    #print(c_val)    \n",
    "    acc = LR_train(1000, c_val, train_dataset, train_labels, test_dataset, test_labels)\n",
    "    accuracy_scores_1.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fdba84684d0>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEACAYAAABcXmojAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdbaca8f7d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(np.arange(.001, 1.0, .005), accuracy_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fdba83635d0>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAD7CAYAAABADhLcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAomIiJREgUREREry/wHpd5oGDXuQHQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdba84a0e50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(np.arange(.021, .032, .0001), accuracy_scores_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max accuracy 0.8604\n",
      "index 37\n",
      "[ 0.024   0.0241  0.0242  0.0243  0.0244  0.0245  0.0246  0.0247  0.0248\n",
      "  0.0249]\n",
      "[0.85999999999999999, 0.86009999999999998, 0.86009999999999998, 0.86009999999999998, 0.86009999999999998, 0.86009999999999998, 0.86019999999999996, 0.86040000000000005, 0.86040000000000005, 0.86040000000000005]\n"
     ]
    }
   ],
   "source": [
    "print('max accuracy', max(accuracy_scores_1))\n",
    "print('index',accuracy_scores_1.index(max(accuracy_scores_1)))\n",
    "print(np.arange(.021,.032,.0001)[30:40])\n",
    "print(accuracy_scores_1[30:40])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Accuracy = .8604  C = .0247, N_samples = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN Training and Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set high-level variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=128\n",
    "IMAGE_PIXELS=28*28\n",
    "hidden1_units=1024\n",
    "hidden2_units=1024\n",
    "hidden3_units=1024\n",
    "NUM_CLASSES=10\n",
    "learning_rate = 0.5\n",
    "max_steps = 3001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build the Graph\n",
    "Questions:  \n",
    "* ** eval_correct: ** Set up eval_correct tensor to call in order to periodically evalulate againstt my validation set.  What gets execcuted in the graph when I do this?  Is it just simply comparing the calculated logits against the responses?  Or is it now using the validated set to continue training the graph?\n",
    "* ** tensor states: ** How to peer into each tensor to view it's values?  How to construct breakpoints in the midst of processing to look at individual tensors.\n",
    "* ** train_prediction: ** I created this when running the training data thru the loop and then calculating the prediction based on weights.  However if I run my validation sets thru this, am I calculating a train_prediction as well with the validation set?  How does the graph know not to adjust the weights for this round?\n",
    "* **placeholder_labels and placeholder_labels_int: ** kludge-y\n",
    "* **eval section:** When I use these tensors to check accuracy across entire test set, I noticed that it does not run the logits through the softmax.  Shouldn't it?  It seems that the predictions should always go thru the Softmax?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def placeholder_inputs():\n",
    "    # Originally set the shape for each of these to specifically use batch_size for # rows\n",
    "    # This caused problems when I wanted to run the entire test set to get an accuracy score\n",
    "    # because batch_size was 128 and the test set dimension was much larger (10000, 784)\n",
    "    # then I saw that the tutorial used None instead and that worked for everything\n",
    " \n",
    "    images_placeholder     =tf.placeholder(tf.float32, shape=(None, IMAGE_PIXELS))\n",
    "    labels_placeholder     =tf.placeholder(tf.float32, shape=(None, NUM_CLASSES))\n",
    "    labels_placeholder_int =tf.placeholder(tf.int32, shape=(None)) \n",
    "    \n",
    "    return images_placeholder, labels_placeholder, labels_placeholder_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feed_forward(images_placeholder, dropout):\n",
    "    \n",
    "    with tf.name_scope('hidden1') as scope:\n",
    "        weights = tf.Variable(tf.truncated_normal([IMAGE_PIXELS, hidden1_units], \n",
    "            stddev=1.0 / math.sqrt(float(IMAGE_PIXELS))), name='weights')\n",
    "        biases = tf.Variable(tf.zeros([hidden1_units]), name = 'biases')\n",
    "        hidden1 = tf.nn.relu(tf.matmul(images_placeholder, weights) + biases)\n",
    "        l2reg_1 = tf.nn.l2_loss(weights)\n",
    "        \n",
    "    with tf.name_scope('hidden2') as scope:\n",
    "        weights = tf.Variable(tf.truncated_normal([hidden1_units, hidden2_units],\n",
    "            stddev=1.0 / math.sqrt(float(hidden1_units))), name='weights')\n",
    "        biases = tf.Variable(tf.zeros([hidden2_units]), name='biases')\n",
    "        hidden2 = tf.nn.relu(tf.matmul(hidden1, weights) + biases)\n",
    "        l2reg_2 = tf.nn.l2_loss(weights)\n",
    "        \n",
    "    #with tf.name_scope('hidden3') as scope:\n",
    "    #    weights = tf.Variable(tf.truncated_normal([hidden2_units, hidden3_units],\n",
    "    #        stddev=1.0 / math.sqrt(float(hidden2_units))), name='weights')\n",
    "    #    biases = tf.Variable(tf.zeros([hidden3_units]), name='biases')\n",
    "    #    hidden3 = tf.nn.relu(tf.matmul(hidden2, weights) + biases)\n",
    "    #    l2reg_3 = tf.nn.l2_loss(weights)\n",
    "\n",
    "    with tf.name_scope('softmax_linear') as scope:\n",
    "        weights = tf.Variable(tf.truncated_normal([hidden2_units, NUM_CLASSES],\n",
    "            stddev=1.0 / math.sqrt(float(hidden2_units))),name='weights')\n",
    "        biases = tf.Variable(tf.zeros([NUM_CLASSES]),name='biases')\n",
    "        l2reg_3 = tf.nn.l2_loss(weights)\n",
    "        \n",
    "        logits = tf.matmul(hidden2, weights) + biases        \n",
    "        if dropout == True:\n",
    "            logits = tf.nn.dropout(logits, 0.5)       \n",
    "        \n",
    "            \n",
    "    return logits, l2reg_1 + l2reg_2 + l2reg_3\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluation(logits, labels_placeholder, labels_placeholder_int):\n",
    "    # Should be (NxNUM_CLASSES)\n",
    "    \n",
    "    # Gets the index of the largest value across the tensor, test_a is prediction, test_b is response\n",
    "    pred = tf.argmax(logits,1)\n",
    "    resp = tf.argmax(labels_placeholder,1)\n",
    "    # Boolean tensor\n",
    "    correct_prediction = tf.equal(resp, pred)\n",
    "    # Get mean across tensor, ie sum/length\n",
    "    eval_acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    # in_top_k requires labels_placeholder to be int32 or int64 list\n",
    "    correct = tf.nn.in_top_k(logits, labels_placeholder_int, 1)\n",
    "    eval_correct = tf.reduce_sum(tf.cast(correct, tf.int32))    \n",
    "    \n",
    "    return(eval_acc, eval_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fill_feed_dict(dataset, labels, labels_int, images_placeholder, labels_placeholder, labels_placeholder_int):\n",
    "    feed_dict = {\n",
    "        images_placeholder : dataset,\n",
    "        labels_placeholder : labels,\n",
    "        labels_placeholder_int : labels_int\n",
    "    }\n",
    "    \n",
    "    return feed_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def inference(l2_hyp, dropout = False):\n",
    "    l2_hyperparm = l2_hyp #.0011\n",
    "    graphn = tf.Graph()\n",
    "\n",
    "    with graphn.as_default():\n",
    "        # Inputs and PLACEHOLDERS\n",
    "        images_placeholder, labels_placeholder, labels_placeholder_int = placeholder_inputs()\n",
    "   \n",
    "        # INFERENCE\n",
    "        logits, l2reg = feed_forward(images_placeholder, dropout)\n",
    "        \n",
    "        # LOSS\n",
    "        # softmax_cross_entropy needs labels_placeholder to be float32 1hot encoded\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, labels_placeholder))\n",
    "        loss = loss + (l2_hyperparm * l2reg)\n",
    "    \n",
    "        # TRAINING\n",
    "        global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "        train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "        # EVAL (use either eval_acc op or the eval_correct op)\n",
    "        eval_acc, eval_correct = evaluation(logits, labels_placeholder, labels_placeholder_int)\n",
    "        \n",
    "    with tf.Session(graph=graphn) as session:\n",
    "        init = tf.initialize_all_variables().run()\n",
    "    \n",
    "        for step in xrange(max_steps):\n",
    "            # Generates the starting index from train_dataset to extract the minibatch\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)        \n",
    "            feed_dict = fill_feed_dict(train_dataset_1h[offset:(offset + batch_size), :], train_labels_1h[offset:(offset + batch_size), :],\n",
    "                train_labels[offset:(offset + batch_size)], images_placeholder, labels_placeholder, labels_placeholder_int)\n",
    "            # A) The fact that I only feed it labels_placeholder and not labels_placeholder_int?\n",
    "            _, l, predictions, num_correct = session.run([train_op, loss, train_prediction, eval_correct], feed_dict=feed_dict)  \n",
    "       \n",
    "            if (step % 500 == 0):\n",
    "                print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "                print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, train_labels_1h[offset:(offset + batch_size), :]))\n",
    "                # Evaluate the entire dataset in specified batch-size increments, return all predictions\n",
    "                acc, eval_predictions = do_validation_eval(session,eval_correct, images_placeholder,labels_placeholder,labels_placeholder_int)\n",
    "                print(\"Validation accuracy: %4.6f\" %(acc))\n",
    "    \n",
    "        fdict = fill_feed_dict(test_dataset_1h, test_labels_1h, test_labels, images_placeholder, labels_placeholder, labels_placeholder_int)\n",
    "        loss, eval_correct, test_acc = session.run([loss, eval_correct, eval_acc], feed_dict=fdict)\n",
    "        print('Test dataset accuracy %4.2f%%' %(100*eval_correct/float(len(test_dataset))))  \n",
    "        \n",
    "    return 100*eval_correct/float(len(test_dataset))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training Loop\n",
    "Questions:\n",
    "* **feed_dict:** See A) comment above. What does it mean if I don't include one of the placeholders in the graph?  Seems like that tensor just does not get called?  If I call that tensor, I have to includ the right placeholder or it chokes\n",
    "* ** Mini batches:** When it is appropriate to do mini-batches?  Is that just for training?  What if I just want to simply predict (and evaluate)?  Why do I need to do this as a mini-batch?\n",
    "* **train vs. test**: How does the graph know when you are training it to update weights, vs just feeding it a data set to calculate accuracy (and not change weights or bias values)?  Would you ever want to push the test data into smaller batch sizes?  In the exmaple below, I did not do that... Is it that if you don't call the train_op vector which tries to minimize loss which would then update the weights and biases?  But when I removed that from my final test run, I did not notice any difference in errors from when I did call for loss.\n",
    "* **Graph 'hierarchy'**: Acc to video, leaf nodes learn edges, middle layer learns high order features (like geom. shapes) and then even higher order objects themselves (like a face or face of cat).  What would \"audit logs\" or text look like?\n",
    "* **Amount of data**: How much data do I need to train our NN?\n",
    "* **Regularization and Loss**: One layer vs. Two layers\n",
    "\n",
    "Note:  \n",
    "* Validation accuracy: 89.703526 and Test dataset accuracy 95.09% without L2 Regularization (2 hidden layers)\n",
    "* Validation accuracy: 88.631811 and Test dataset accuracy 94.21% (1 hidden layer)\n",
    "* With 6e-10 regularization hyperparameter, test accuracy only improved 94.24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def do_validation_eval(session, eval_correct, images_placeholder, labels_placeholder, labels_placeholder_int):\n",
    "    true_count = 0  # Counts the number of correct predictions.\n",
    "    steps_per_epoch = valid_dataset.shape[0] // batch_size\n",
    "    num_examples = steps_per_epoch * batch_size\n",
    "    eval_predictions = []\n",
    "    for step in xrange(steps_per_epoch):\n",
    "        eval_offset = (step * batch_size) % (valid_labels.shape[0] - batch_size)\n",
    "        f_dict = fill_feed_dict(valid_dataset_1h[eval_offset:(eval_offset + batch_size), :], valid_labels_1h[eval_offset:(eval_offset + batch_size), :], \n",
    "            valid_labels[eval_offset:(eval_offset + batch_size)], images_placeholder, labels_placeholder, labels_placeholder_int)\n",
    "        eval_predictions += session.run([eval_correct], feed_dict=f_dict)\n",
    "\n",
    "    return 100*sum(eval_predictions)/float(num_examples), eval_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BETA value: 0.0\n",
      "Minibatch loss at step 0: 2.360609\n",
      "Minibatch accuracy: 5.5%\n",
      "Validation accuracy: 38.561699\n",
      "Minibatch loss at step 500: 0.486144\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 84.915865\n",
      "Minibatch loss at step 1000: 0.327002\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 86.177885\n",
      "Minibatch loss at step 1500: 0.453064\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 87.239583\n",
      "Minibatch loss at step 2000: 0.277680\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 87.550080\n",
      "Minibatch loss at step 2500: 0.366509\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 87.900641\n",
      "Minibatch loss at step 3000: 0.264751\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 88.752003\n",
      "Test dataset accuracy 94.16%\n",
      "BETA value: 2e-10\n",
      "Minibatch loss at step 0: 2.371467\n",
      "Minibatch accuracy: 9.4%\n",
      "Validation accuracy: 45.072115\n",
      "Minibatch loss at step 500: 0.481893\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 84.605369\n",
      "Minibatch loss at step 1000: 0.332284\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 85.576923\n",
      "Minibatch loss at step 1500: 0.450426\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 87.359776\n",
      "Minibatch loss at step 2000: 0.296021\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 87.630208\n",
      "Minibatch loss at step 2500: 0.370246\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.840545\n",
      "Minibatch loss at step 3000: 0.266891\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 88.561699\n",
      "Test dataset accuracy 94.03%\n",
      "BETA value: 4e-10\n",
      "Minibatch loss at step 0: 2.347673\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 40.294471\n",
      "Minibatch loss at step 500: 0.490329\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 85.206330\n",
      "Minibatch loss at step 1000: 0.324577\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 86.147837\n",
      "Minibatch loss at step 1500: 0.425975\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 87.409856\n",
      "Minibatch loss at step 2000: 0.266775\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 87.850561\n",
      "Minibatch loss at step 2500: 0.340544\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 87.820513\n",
      "Minibatch loss at step 3000: 0.271625\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 88.371394\n",
      "Test dataset accuracy 93.93%\n",
      "BETA value: 6e-10\n",
      "Minibatch loss at step 0: 2.285122\n",
      "Minibatch accuracy: 9.4%\n",
      "Validation accuracy: 41.856971\n",
      "Minibatch loss at step 500: 0.496485\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 84.905849\n",
      "Minibatch loss at step 1000: 0.338907\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 86.127804\n",
      "Minibatch loss at step 1500: 0.470618\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 87.339744\n",
      "Minibatch loss at step 2000: 0.269453\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 87.780449\n",
      "Minibatch loss at step 2500: 0.376309\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 86.688702\n",
      "Minibatch loss at step 3000: 0.254935\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 88.591747\n",
      "Test dataset accuracy 94.26%\n",
      "BETA value: 8e-10\n",
      "Minibatch loss at step 0: 2.358694\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 39.593349\n",
      "Minibatch loss at step 500: 0.477952\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 84.985978\n",
      "Minibatch loss at step 1000: 0.359099\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 85.827324\n",
      "Minibatch loss at step 1500: 0.434261\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 87.299679\n",
      "Minibatch loss at step 2000: 0.272392\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 87.730369\n",
      "Minibatch loss at step 2500: 0.362132\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 87.900641\n",
      "Minibatch loss at step 3000: 0.264476\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 88.882212\n",
      "Test dataset accuracy 94.21%\n",
      "BETA value: 1e-09\n",
      "Minibatch loss at step 0: 2.345975\n",
      "Minibatch accuracy: 4.7%\n",
      "Validation accuracy: 41.536458\n",
      "Minibatch loss at step 500: 0.468749\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 85.036058\n",
      "Minibatch loss at step 1000: 0.348723\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 85.987580\n",
      "Minibatch loss at step 1500: 0.405071\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 87.510016\n",
      "Minibatch loss at step 2000: 0.265864\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 87.760417\n",
      "Minibatch loss at step 2500: 0.355779\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 87.870593\n",
      "Minibatch loss at step 3000: 0.282757\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 88.591747\n",
      "Test dataset accuracy 94.09%\n",
      "BETA value: 1.2e-09\n",
      "Minibatch loss at step 0: 2.354931\n",
      "Minibatch accuracy: 4.7%\n",
      "Validation accuracy: 38.271234\n",
      "Minibatch loss at step 500: 0.482758\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 84.975962\n",
      "Minibatch loss at step 1000: 0.340395\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 86.187901\n",
      "Minibatch loss at step 1500: 0.434966\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 87.339744\n",
      "Minibatch loss at step 2000: 0.283972\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 87.670272\n",
      "Minibatch loss at step 2500: 0.366009\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 87.870593\n",
      "Minibatch loss at step 3000: 0.251800\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 88.701923\n",
      "Test dataset accuracy 94.22%\n",
      "BETA value: 1.4e-09\n",
      "Minibatch loss at step 0: 2.291960\n",
      "Minibatch accuracy: 14.1%\n",
      "Validation accuracy: 52.403846\n",
      "Minibatch loss at step 500: 0.459262\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 85.066106\n",
      "Minibatch loss at step 1000: 0.337833\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 86.258013\n",
      "Minibatch loss at step 1500: 0.437398\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 87.229567\n",
      "Minibatch loss at step 2000: 0.246836\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 87.800481\n",
      "Minibatch loss at step 2500: 0.353805\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 86.748798\n",
      "Minibatch loss at step 3000: 0.250507\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 88.601763\n",
      "Test dataset accuracy 94.05%\n",
      "BETA value: 1.6e-09\n",
      "Minibatch loss at step 0: 2.298117\n",
      "Minibatch accuracy: 19.5%\n",
      "Validation accuracy: 41.616587\n",
      "Minibatch loss at step 500: 0.498521\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 84.855769\n",
      "Minibatch loss at step 1000: 0.352344\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 85.817308\n",
      "Minibatch loss at step 1500: 0.429541\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 87.329728\n",
      "Minibatch loss at step 2000: 0.277532\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 87.740385\n",
      "Minibatch loss at step 2500: 0.343475\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 87.900641\n",
      "Minibatch loss at step 3000: 0.264778\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 88.641827\n",
      "Test dataset accuracy 94.20%\n",
      "BETA value: 1.8e-09\n",
      "Minibatch loss at step 0: 2.318843\n",
      "Minibatch accuracy: 5.5%\n",
      "Validation accuracy: 45.022035\n",
      "Minibatch loss at step 500: 0.472777\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 85.006010\n",
      "Minibatch loss at step 1000: 0.343989\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 85.947516\n",
      "Minibatch loss at step 1500: 0.418582\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 87.510016\n",
      "Minibatch loss at step 2000: 0.291525\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 87.670272\n",
      "Minibatch loss at step 2500: 0.354061\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 87.910657\n",
      "Minibatch loss at step 3000: 0.277605\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 88.501603\n",
      "Test dataset accuracy 94.03%\n",
      "BETA value: 2e-09\n",
      "Minibatch loss at step 0: 2.379960\n",
      "Minibatch accuracy: 4.7%\n",
      "Validation accuracy: 39.593349\n",
      "Minibatch loss at step 500: 0.481089\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 85.126202\n",
      "Minibatch loss at step 1000: 0.350237\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 85.877404\n",
      "Minibatch loss at step 1500: 0.423729\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 87.279647\n",
      "Minibatch loss at step 2000: 0.278273\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 87.489984\n",
      "Minibatch loss at step 2500: 0.344578\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 88.030849\n",
      "Minibatch loss at step 3000: 0.288352\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 88.681891\n",
      "Test dataset accuracy 94.18%\n",
      "BETA value: 2.2e-09\n",
      "Minibatch loss at step 0: 2.316231\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 42.718349\n",
      "Minibatch loss at step 500: 0.479174\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 84.865785\n",
      "Minibatch loss at step 1000: 0.344661\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 86.268029\n",
      "Minibatch loss at step 1500: 0.439418\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 87.560096\n",
      "Minibatch loss at step 2000: 0.274099\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 87.790465\n",
      "Minibatch loss at step 2500: 0.354322\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 87.960737\n",
      "Minibatch loss at step 3000: 0.276497\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.022436\n",
      "Test dataset accuracy 94.21%\n",
      "BETA value: 2.4e-09\n",
      "Minibatch loss at step 0: 2.349767\n",
      "Minibatch accuracy: 9.4%\n",
      "Validation accuracy: 39.042468\n",
      "Minibatch loss at step 500: 0.475382\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 84.945913\n",
      "Minibatch loss at step 1000: 0.348631\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 85.807292\n",
      "Minibatch loss at step 1500: 0.416231\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 87.249599\n",
      "Minibatch loss at step 2000: 0.280053\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 87.690304\n",
      "Minibatch loss at step 2500: 0.361764\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 88.060897\n",
      "Minibatch loss at step 3000: 0.258491\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 88.571715\n",
      "Test dataset accuracy 94.22%\n",
      "BETA value: 2.6e-09\n",
      "Minibatch loss at step 0: 2.355852\n",
      "Minibatch accuracy: 9.4%\n",
      "Validation accuracy: 38.040865\n",
      "Minibatch loss at step 500: 0.483569\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 84.765625\n",
      "Minibatch loss at step 1000: 0.343209\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 86.007612\n",
      "Minibatch loss at step 1500: 0.430981\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 87.339744\n",
      "Minibatch loss at step 2000: 0.260645\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 87.760417\n",
      "Minibatch loss at step 2500: 0.355512\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 87.820513\n",
      "Minibatch loss at step 3000: 0.262158\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 88.651843\n",
      "Test dataset accuracy 94.01%\n",
      "BETA value: 2.8e-09\n",
      "Minibatch loss at step 0: 2.343009\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 44.280849\n",
      "Minibatch loss at step 500: 0.483897\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 84.735577\n",
      "Minibatch loss at step 1000: 0.352943\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 85.897436\n",
      "Minibatch loss at step 1500: 0.426806\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 87.459936\n",
      "Minibatch loss at step 2000: 0.265444\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 87.690304\n",
      "Minibatch loss at step 2500: 0.343472\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 87.890625\n",
      "Minibatch loss at step 3000: 0.241066\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 88.812099\n",
      "Test dataset accuracy 94.16%\n",
      "BETA value: 3e-09\n",
      "Minibatch loss at step 0: 2.297117\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 40.244391\n",
      "Minibatch loss at step 500: 0.474356\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 85.026042\n",
      "Minibatch loss at step 1000: 0.370467\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 86.097756\n",
      "Minibatch loss at step 1500: 0.410432\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 87.510016\n",
      "Minibatch loss at step 2000: 0.293096\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 87.570112\n",
      "Minibatch loss at step 2500: 0.354364\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 88.110978\n",
      "Minibatch loss at step 3000: 0.263017\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 88.661859\n",
      "Test dataset accuracy 94.04%\n",
      "BETA value: 3.2e-09\n",
      "Minibatch loss at step 0: 2.378958\n",
      "Minibatch accuracy: 7.8%\n",
      "Validation accuracy: 39.503205\n",
      "Minibatch loss at step 500: 0.479056\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 84.955929\n",
      "Minibatch loss at step 1000: 0.361374\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 85.957532\n",
      "Minibatch loss at step 1500: 0.425550\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 87.129407\n",
      "Minibatch loss at step 2000: 0.297880\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 87.650240\n",
      "Minibatch loss at step 2500: 0.360967\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 87.860577\n",
      "Minibatch loss at step 3000: 0.277282\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 88.411458\n",
      "Test dataset accuracy 93.93%\n",
      "BETA value: 3.4e-09\n",
      "Minibatch loss at step 0: 2.347344\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 41.736779\n",
      "Minibatch loss at step 500: 0.495213\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 84.875801\n",
      "Minibatch loss at step 1000: 0.351294\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.067708\n",
      "Minibatch loss at step 1500: 0.430456\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 87.249599\n",
      "Minibatch loss at step 2000: 0.290779\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 87.750401\n",
      "Minibatch loss at step 2500: 0.327187\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 88.040865\n",
      "Minibatch loss at step 3000: 0.258012\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 88.752003\n",
      "Test dataset accuracy 94.15%\n",
      "BETA value: 3.6e-09\n",
      "Minibatch loss at step 0: 2.376389\n",
      "Minibatch accuracy: 7.0%\n",
      "Validation accuracy: 39.523237\n",
      "Minibatch loss at step 500: 0.497685\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 84.945913\n",
      "Minibatch loss at step 1000: 0.342273\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 85.897436\n",
      "Minibatch loss at step 1500: 0.432749\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 87.139423\n",
      "Minibatch loss at step 2000: 0.284420\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 87.680288\n",
      "Minibatch loss at step 2500: 0.361749\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 87.820513\n",
      "Minibatch loss at step 3000: 0.276885\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 88.621795\n",
      "Test dataset accuracy 93.92%\n",
      "BETA value: 3.8e-09\n",
      "Minibatch loss at step 0: 2.287153\n",
      "Minibatch accuracy: 15.6%\n",
      "Validation accuracy: 40.534856\n",
      "Minibatch loss at step 500: 0.504400\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 85.006010\n",
      "Minibatch loss at step 1000: 0.356236\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 85.907452\n",
      "Minibatch loss at step 1500: 0.439565\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 87.369792\n",
      "Minibatch loss at step 2000: 0.263448\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 87.890625\n",
      "Minibatch loss at step 2500: 0.357184\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 86.708734\n",
      "Minibatch loss at step 3000: 0.271567\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 88.962340\n",
      "Test dataset accuracy 94.23%\n",
      "BETA value: 4e-09\n",
      "Minibatch loss at step 0: 2.270888\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 42.407853\n",
      "Minibatch loss at step 500: 0.494209\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 84.975962\n",
      "Minibatch loss at step 1000: 0.341433\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 85.977564\n",
      "Minibatch loss at step 1500: 0.423728\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 87.359776\n",
      "Minibatch loss at step 2000: 0.279162\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 87.550080\n",
      "Minibatch loss at step 2500: 0.348276\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 87.770433\n",
      "Minibatch loss at step 3000: 0.261580\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 88.792067\n",
      "Test dataset accuracy 94.07%\n",
      "BETA value: 4.2e-09\n",
      "Minibatch loss at step 0: 2.349950\n",
      "Minibatch accuracy: 9.4%\n",
      "Validation accuracy: 44.270833\n",
      "Minibatch loss at step 500: 0.481147\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 84.655449\n",
      "Minibatch loss at step 1000: 0.365252\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 85.957532\n",
      "Minibatch loss at step 1500: 0.447421\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 87.419872\n",
      "Minibatch loss at step 2000: 0.286492\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 87.650240\n",
      "Minibatch loss at step 2500: 0.341926\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 87.750401\n",
      "Minibatch loss at step 3000: 0.265904\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 88.691907\n",
      "Test dataset accuracy 94.12%\n",
      "BETA value: 4.4e-09\n",
      "Minibatch loss at step 0: 2.313927\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 40.394631\n",
      "Minibatch loss at step 500: 0.490768\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 85.126202\n",
      "Minibatch loss at step 1000: 0.343396\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 86.067708\n",
      "Minibatch loss at step 1500: 0.439517\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 87.449920\n",
      "Minibatch loss at step 2000: 0.281825\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 87.700321\n",
      "Minibatch loss at step 2500: 0.355111\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 87.900641\n",
      "Minibatch loss at step 3000: 0.277039\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 88.571715\n",
      "Test dataset accuracy 93.91%\n",
      "BETA value: 4.6e-09\n",
      "Minibatch loss at step 0: 2.339904\n",
      "Minibatch accuracy: 16.4%\n",
      "Validation accuracy: 45.452724\n",
      "Minibatch loss at step 500: 0.489760\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 84.755609\n",
      "Minibatch loss at step 1000: 0.357732\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 85.717147\n",
      "Minibatch loss at step 1500: 0.426988\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 87.319712\n",
      "Minibatch loss at step 2000: 0.278350\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 87.580128\n",
      "Minibatch loss at step 2500: 0.355072\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 87.820513\n",
      "Minibatch loss at step 3000: 0.256871\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 88.601763\n",
      "Test dataset accuracy 94.05%\n",
      "BETA value: 4.8e-09\n",
      "Minibatch loss at step 0: 2.317897\n",
      "Minibatch accuracy: 4.7%\n",
      "Validation accuracy: 42.327724\n",
      "Minibatch loss at step 500: 0.478538\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 84.745593\n",
      "Minibatch loss at step 1000: 0.338555\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 85.637019\n",
      "Minibatch loss at step 1500: 0.431828\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 86.989183\n",
      "Minibatch loss at step 2000: 0.277188\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 87.760417\n",
      "Minibatch loss at step 2500: 0.345597\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 87.770433\n",
      "Minibatch loss at step 3000: 0.284145\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 88.822115\n",
      "Test dataset accuracy 94.10%\n",
      "BETA value: 5e-09\n",
      "Minibatch loss at step 0: 2.356735\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 43.159054\n",
      "Minibatch loss at step 500: 0.492561\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 85.216346\n",
      "Minibatch loss at step 1000: 0.341105\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 86.127804\n",
      "Minibatch loss at step 1500: 0.444139\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 87.149439\n",
      "Minibatch loss at step 2000: 0.297802\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 87.520032\n",
      "Minibatch loss at step 2500: 0.358635\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 87.760417\n",
      "Minibatch loss at step 3000: 0.255482\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 88.782051\n",
      "Test dataset accuracy 94.18%\n"
     ]
    }
   ],
   "source": [
    "predacc_scores = []\n",
    "for multiplier in range(0, 26):\n",
    "    beta = 1e-10 * (2*multiplier)\n",
    "    print('BETA value:', beta)\n",
    "    predacc_scores.append((beta, inference(beta)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fdb938e8f10>]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAENCAYAAADpK9mHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXJcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdb93920710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot([beta for beta,_ in predacc_scores], [acc for beta, acc in predacc_scores])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6e-10, 94.260000000000005)\n"
     ]
    }
   ],
   "source": [
    "l = [acc for beta, acc in predacc_scores]\n",
    "print(predacc_scores[l.index(max(l))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---\n",
    "* 300 training samples total (repeated over and over) with more features (ie 784).\n",
    "* Minibatch Training accuracy reaches 100%\n",
    "* However testing accuracy and validation quite a bit lower (why is testing accuracy consistently higher than validation?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set-1H (300, 784) (300, 10)\n",
      "Training set (300, 28, 28) (300,)\n",
      "Validation set-1H (10000, 784) (10000, 10)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set-1H (10000, 784) (10000, 10)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Current Training set sizes\n",
    "print('Training set-1H', train_dataset_1h.shape, train_labels_1h.shape)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set-1H', valid_dataset_1h.shape, valid_labels_1h.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set-1H', test_dataset_1h.shape, test_labels_1h.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reduce Training set sizes to just 300 to force OVERFITTING\n",
    "train_dataset_1h = train_dataset_1h[0:300]\n",
    "train_dataset = train_dataset[0:300]\n",
    "train_labels_1h = train_labels_1h[0:300]\n",
    "train_labels = train_labels[0:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 0: 2.266426\n",
      "Minibatch accuracy: 11.7%\n",
      "Validation accuracy: 40.665064\n",
      "Minibatch loss at step 500: 0.001863\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 75.961538\n",
      "Minibatch loss at step 1000: 0.000626\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.001603\n",
      "Minibatch loss at step 1500: 0.000345\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 75.941506\n",
      "Minibatch loss at step 2000: 0.000230\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 75.991587\n",
      "Minibatch loss at step 2500: 0.000171\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.011619\n",
      "Minibatch loss at step 3000: 0.000142\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.041667\n",
      "Test dataset accuracy 83.35%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "83.349999999999994"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference(6e-10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---\n",
    "* Training accuracy per minibatch is lower with dropouts\n",
    "* However Validation accuracy increased 5-6 percentage points from previous run\n",
    "* Testing accuracy also improved "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 0: 2.397096\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 27.744391\n",
      "Minibatch loss at step 500: 0.859668\n",
      "Minibatch accuracy: 58.6%\n",
      "Validation accuracy: 81.510417\n",
      "Minibatch loss at step 1000: 0.871901\n",
      "Minibatch accuracy: 57.0%\n",
      "Validation accuracy: 81.810897\n",
      "Minibatch loss at step 1500: 0.778105\n",
      "Minibatch accuracy: 58.6%\n",
      "Validation accuracy: 82.081330\n",
      "Minibatch loss at step 2000: 0.855699\n",
      "Minibatch accuracy: 57.8%\n",
      "Validation accuracy: 81.750801\n",
      "Minibatch loss at step 2500: 0.786091\n",
      "Minibatch accuracy: 58.6%\n",
      "Validation accuracy: 82.271635\n",
      "Minibatch loss at step 3000: 0.950642\n",
      "Minibatch accuracy: 53.1%\n",
      "Validation accuracy: 82.231571\n",
      "Test dataset accuracy 86.99%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "86.989999999999995"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference(6e-06, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n",
    "Not sure if this is right:\n",
    "- 1) Add new hidden layer\n",
    "- 2) Add L2 regularization weights to the total weights\n",
    "- 3) Find optimal L2 (no drop-out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 0: 2.345007\n",
      "Minibatch accuracy: 9.4%\n",
      "Validation accuracy: 42.317708\n",
      "Minibatch loss at step 500: 0.487481\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 85.026042\n",
      "Minibatch loss at step 1000: 0.364757\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 86.107772\n",
      "Minibatch loss at step 1500: 0.440881\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 87.359776\n",
      "Minibatch loss at step 2000: 0.296254\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 87.850561\n",
      "Minibatch loss at step 2500: 0.354589\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 87.910657\n",
      "Minibatch loss at step 3000: 0.260303\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 88.822115\n",
      "Test dataset accuracy 94.18%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "94.180000000000007"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 hidden layers with no drop-out and previous regularization (94.18%)\n",
    "inference(6e-06)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimize  L2 regularization (no drop-out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BETA value: 1e-10\n",
      "Minibatch loss at step 0: 2.337105\n",
      "Minibatch accuracy: 3.9%\n",
      "Validation accuracy: 35.526843\n",
      "Minibatch loss at step 500: 0.429992\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 85.126202\n",
      "Minibatch loss at step 1000: 0.331128\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 87.119391\n",
      "Minibatch loss at step 1500: 0.425749\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 87.950721\n",
      "Minibatch loss at step 2000: 0.248364\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.052484\n",
      "Minibatch loss at step 2500: 0.279115\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.513221\n",
      "Minibatch loss at step 3000: 0.206842\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 89.803686\n",
      "Test dataset accuracy 95.04%\n",
      "BETA value: 1e-09\n",
      "Minibatch loss at step 0: 2.291290\n",
      "Minibatch accuracy: 14.1%\n",
      "Validation accuracy: 35.837340\n",
      "Minibatch loss at step 500: 0.440145\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 84.985978\n",
      "Minibatch loss at step 1000: 0.349239\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 87.149439\n",
      "Minibatch loss at step 1500: 0.432019\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 88.100962\n",
      "Minibatch loss at step 2000: 0.231134\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 88.912260\n",
      "Minibatch loss at step 2500: 0.314592\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.483173\n",
      "Minibatch loss at step 3000: 0.224007\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.453125\n",
      "Test dataset accuracy 94.87%\n",
      "BETA value: 1e-08\n",
      "Minibatch loss at step 0: 2.307790\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 32.041266\n",
      "Minibatch loss at step 500: 0.423579\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 84.985978\n",
      "Minibatch loss at step 1000: 0.360212\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 86.909054\n",
      "Minibatch loss at step 1500: 0.421497\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 88.131010\n",
      "Minibatch loss at step 2000: 0.233465\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.272837\n",
      "Minibatch loss at step 2500: 0.321323\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 89.453125\n",
      "Minibatch loss at step 3000: 0.218585\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 89.863782\n",
      "Test dataset accuracy 95.10%\n",
      "BETA value: 1e-07\n",
      "Minibatch loss at step 0: 2.322301\n",
      "Minibatch accuracy: 7.0%\n",
      "Validation accuracy: 33.854167\n",
      "Minibatch loss at step 500: 0.428249\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 84.755609\n",
      "Minibatch loss at step 1000: 0.313771\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 87.019231\n",
      "Minibatch loss at step 1500: 0.428287\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 88.191106\n",
      "Minibatch loss at step 2000: 0.218548\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 88.872196\n",
      "Minibatch loss at step 2500: 0.310732\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 89.372997\n",
      "Minibatch loss at step 3000: 0.199980\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.833734\n",
      "Test dataset accuracy 95.11%\n",
      "BETA value: 1e-06\n",
      "Minibatch loss at step 0: 2.307573\n",
      "Minibatch accuracy: 16.4%\n",
      "Validation accuracy: 33.453526\n",
      "Minibatch loss at step 500: 0.449875\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 84.895833\n",
      "Minibatch loss at step 1000: 0.329921\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 86.939103\n",
      "Minibatch loss at step 1500: 0.441600\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 88.120994\n",
      "Minibatch loss at step 2000: 0.239145\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 88.932292\n",
      "Minibatch loss at step 2500: 0.287677\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.433093\n",
      "Minibatch loss at step 3000: 0.210008\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 89.503205\n",
      "Test dataset accuracy 94.92%\n",
      "BETA value: 1e-05\n",
      "Minibatch loss at step 0: 2.294765\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 32.662260\n",
      "Minibatch loss at step 500: 0.437061\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 84.765625\n",
      "Minibatch loss at step 1000: 0.317026\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 86.969151\n",
      "Minibatch loss at step 1500: 0.453330\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 88.191106\n",
      "Minibatch loss at step 2000: 0.228376\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.052484\n",
      "Minibatch loss at step 2500: 0.309659\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.423077\n",
      "Minibatch loss at step 3000: 0.212911\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 89.513221\n",
      "Test dataset accuracy 94.94%\n",
      "BETA value: 0.0001\n",
      "Minibatch loss at step 0: 2.386283\n",
      "Minibatch accuracy: 11.7%\n",
      "Validation accuracy: 39.663462\n",
      "Minibatch loss at step 500: 0.517537\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 85.116186\n",
      "Minibatch loss at step 1000: 0.426625\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 87.159455\n",
      "Minibatch loss at step 1500: 0.507695\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 88.261218\n",
      "Minibatch loss at step 2000: 0.327614\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 88.952324\n",
      "Minibatch loss at step 2500: 0.367005\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 89.302885\n",
      "Minibatch loss at step 3000: 0.304608\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 89.723558\n",
      "Test dataset accuracy 94.91%\n",
      "BETA value: 0.001\n",
      "Minibatch loss at step 0: 3.117230\n",
      "Minibatch accuracy: 7.0%\n",
      "Validation accuracy: 41.646635\n",
      "Minibatch loss at step 500: 0.975674\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 84.625401\n",
      "Minibatch loss at step 1000: 0.721802\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 86.378205\n",
      "Minibatch loss at step 1500: 0.696247\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 87.339744\n",
      "Minibatch loss at step 2000: 0.473653\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 87.630208\n",
      "Minibatch loss at step 2500: 0.516684\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.000801\n",
      "Minibatch loss at step 3000: 0.408279\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 88.301282\n",
      "Test dataset accuracy 93.83%\n",
      "BETA value: 0.01\n",
      "Minibatch loss at step 0: 10.267067\n",
      "Minibatch accuracy: 9.4%\n",
      "Validation accuracy: 33.353365\n",
      "Minibatch loss at step 500: 0.841901\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 81.270032\n",
      "Minibatch loss at step 1000: 0.705047\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 82.161458\n",
      "Minibatch loss at step 1500: 0.767441\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 83.934295\n",
      "Minibatch loss at step 2000: 0.714032\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 79.707532\n",
      "Minibatch loss at step 2500: 0.726749\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 82.982772\n",
      "Minibatch loss at step 3000: 0.685344\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 83.263221\n",
      "Test dataset accuracy 89.89%\n",
      "BETA value: 0.1\n",
      "Minibatch loss at step 0: 81.868042\n",
      "Minibatch accuracy: 4.7%\n",
      "Validation accuracy: 36.999199\n",
      "Minibatch loss at step 500: 1.707550\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 66.826923\n",
      "Minibatch loss at step 1000: 1.625444\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 72.015224\n",
      "Minibatch loss at step 1500: 1.724887\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 74.138622\n",
      "Minibatch loss at step 2000: 1.779383\n",
      "Minibatch accuracy: 71.1%\n",
      "Validation accuracy: 67.638221\n",
      "Minibatch loss at step 2500: 1.685297\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 68.960337\n",
      "Minibatch loss at step 3000: 1.702327\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy: 66.015625\n",
      "Test dataset accuracy 72.54%\n"
     ]
    }
   ],
   "source": [
    "predacc_scores_1 = []\n",
    "for exponent in range(0, 10):\n",
    "    beta = 1e-10 * math.pow(10, exponent)\n",
    "    print('BETA value:', beta)\n",
    "    predacc_scores_1.append((beta, inference(beta)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fdb93906c90>]"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAEACAYAAABbMHZzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ndO9vFAcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdb93b565d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot([beta for beta,_ in predacc_scores_1], [acc for beta, acc in predacc_scores_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1e-10, 95.040000000000006)"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predacc_scores_1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BETA value: 0.0\n",
      "Minibatch loss at step 0: 2.358450\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 31.931090\n",
      "Minibatch loss at step 500: 0.439507\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 84.905849\n",
      "Minibatch loss at step 1000: 0.348553\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 87.029247\n",
      "Minibatch loss at step 1500: 0.436517\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 88.251202\n",
      "Minibatch loss at step 2000: 0.227681\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.082532\n",
      "Minibatch loss at step 2500: 0.302277\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.493189\n",
      "Minibatch loss at step 3000: 0.188560\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.693510\n",
      "Test dataset accuracy 94.95%\n",
      "BETA value: 4e-11\n",
      "Minibatch loss at step 0: 2.350454\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 34.294872\n",
      "Minibatch loss at step 500: 0.441466\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 85.096154\n",
      "Minibatch loss at step 1000: 0.335179\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 86.999199\n",
      "Minibatch loss at step 1500: 0.444341\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 88.110978\n",
      "Minibatch loss at step 2000: 0.252341\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 88.822115\n",
      "Minibatch loss at step 2500: 0.299703\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.322917\n",
      "Minibatch loss at step 3000: 0.197661\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.903846\n",
      "Test dataset accuracy 94.92%\n",
      "BETA value: 8e-11\n",
      "Minibatch loss at step 0: 2.326027\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 35.967548\n",
      "Minibatch loss at step 500: 0.446224\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 84.955929\n",
      "Minibatch loss at step 1000: 0.314934\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 87.019231\n",
      "Minibatch loss at step 1500: 0.467362\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 88.171074\n",
      "Minibatch loss at step 2000: 0.229601\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 88.892228\n",
      "Minibatch loss at step 2500: 0.299999\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.242788\n",
      "Minibatch loss at step 3000: 0.197783\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 89.663462\n",
      "Test dataset accuracy 95.17%\n",
      "BETA value: 1.2e-10\n",
      "Minibatch loss at step 0: 2.323179\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 37.550080\n",
      "Minibatch loss at step 500: 0.442269\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 84.625401\n",
      "Minibatch loss at step 1000: 0.332231\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 87.279647\n",
      "Minibatch loss at step 1500: 0.447687\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 88.281250\n",
      "Minibatch loss at step 2000: 0.232516\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 88.972356\n",
      "Minibatch loss at step 2500: 0.316673\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 89.463141\n",
      "Minibatch loss at step 3000: 0.212165\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.783654\n",
      "Test dataset accuracy 95.01%\n",
      "BETA value: 1.6e-10\n",
      "Minibatch loss at step 0: 2.307835\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 38.962340\n",
      "Minibatch loss at step 500: 0.448747\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 85.246394\n",
      "Minibatch loss at step 1000: 0.352076\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 86.949119\n",
      "Minibatch loss at step 1500: 0.451357\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 88.070913\n",
      "Minibatch loss at step 2000: 0.219726\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 88.972356\n",
      "Minibatch loss at step 2500: 0.275123\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.152644\n",
      "Minibatch loss at step 3000: 0.206793\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 89.553285\n",
      "Test dataset accuracy 94.90%\n",
      "BETA value: 2e-10\n",
      "Minibatch loss at step 0: 2.314992\n",
      "Minibatch accuracy: 13.3%\n",
      "Validation accuracy: 35.076122\n",
      "Minibatch loss at step 500: 0.420651\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 85.236378\n",
      "Minibatch loss at step 1000: 0.330393\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 87.039263\n",
      "Minibatch loss at step 1500: 0.415404\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 88.261218\n",
      "Minibatch loss at step 2000: 0.243958\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 88.912260\n",
      "Minibatch loss at step 2500: 0.280176\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.453125\n",
      "Minibatch loss at step 3000: 0.229424\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.843750\n",
      "Test dataset accuracy 94.91%\n",
      "BETA value: 2.4e-10\n",
      "Minibatch loss at step 0: 2.321398\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 35.486779\n",
      "Minibatch loss at step 500: 0.436936\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 84.875801\n",
      "Minibatch loss at step 1000: 0.337658\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.049279\n",
      "Minibatch loss at step 1500: 0.417166\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 88.161058\n",
      "Minibatch loss at step 2000: 0.240185\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.042468\n",
      "Minibatch loss at step 2500: 0.287457\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 89.393029\n",
      "Minibatch loss at step 3000: 0.201381\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.723558\n",
      "Test dataset accuracy 95.09%\n",
      "BETA value: 2.8e-10\n",
      "Minibatch loss at step 0: 2.291532\n",
      "Minibatch accuracy: 11.7%\n",
      "Validation accuracy: 38.211138\n",
      "Minibatch loss at step 500: 0.443563\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 84.825721\n",
      "Minibatch loss at step 1000: 0.340579\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 86.989183\n",
      "Minibatch loss at step 1500: 0.421131\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 88.090946\n",
      "Minibatch loss at step 2000: 0.230969\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 88.992388\n",
      "Minibatch loss at step 2500: 0.298848\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.372997\n",
      "Minibatch loss at step 3000: 0.210950\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 89.853766\n",
      "Test dataset accuracy 94.90%\n",
      "BETA value: 3.2e-10\n",
      "Minibatch loss at step 0: 2.319519\n",
      "Minibatch accuracy: 13.3%\n",
      "Validation accuracy: 34.845753\n",
      "Minibatch loss at step 500: 0.437941\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 85.176282\n",
      "Minibatch loss at step 1000: 0.326529\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 87.209535\n",
      "Minibatch loss at step 1500: 0.431097\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 88.100962\n",
      "Minibatch loss at step 2000: 0.237036\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 88.942308\n",
      "Minibatch loss at step 2500: 0.320536\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.493189\n",
      "Minibatch loss at step 3000: 0.205850\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 89.663462\n",
      "Test dataset accuracy 95.05%\n",
      "BETA value: 3.6e-10\n",
      "Minibatch loss at step 0: 2.305599\n",
      "Minibatch accuracy: 13.3%\n",
      "Validation accuracy: 44.250801\n",
      "Minibatch loss at step 500: 0.437244\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 84.825721\n",
      "Minibatch loss at step 1000: 0.328480\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 87.379808\n",
      "Minibatch loss at step 1500: 0.430623\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 88.030849\n",
      "Minibatch loss at step 2000: 0.249886\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 88.912260\n",
      "Minibatch loss at step 2500: 0.298356\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.453125\n",
      "Minibatch loss at step 3000: 0.209217\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.663462\n",
      "Test dataset accuracy 94.92%\n",
      "BETA value: 4e-10\n",
      "Minibatch loss at step 0: 2.314370\n",
      "Minibatch accuracy: 9.4%\n",
      "Validation accuracy: 32.612179\n",
      "Minibatch loss at step 500: 0.441676\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 85.196314\n",
      "Minibatch loss at step 1000: 0.335424\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 87.129407\n",
      "Minibatch loss at step 1500: 0.442436\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 88.120994\n",
      "Minibatch loss at step 2000: 0.234597\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 88.872196\n",
      "Minibatch loss at step 2500: 0.296631\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.342949\n",
      "Minibatch loss at step 3000: 0.214435\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 89.703526\n",
      "Test dataset accuracy 94.86%\n",
      "BETA value: 4.4e-10\n",
      "Minibatch loss at step 0: 2.336013\n",
      "Minibatch accuracy: 7.8%\n",
      "Validation accuracy: 32.431891\n",
      "Minibatch loss at step 500: 0.440834\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 85.406651\n",
      "Minibatch loss at step 1000: 0.327348\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.339744\n",
      "Minibatch loss at step 1500: 0.455209\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 88.241186\n",
      "Minibatch loss at step 2000: 0.228663\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.042468\n",
      "Minibatch loss at step 2500: 0.299676\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.342949\n",
      "Minibatch loss at step 3000: 0.204146\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 89.703526\n",
      "Test dataset accuracy 94.88%\n",
      "BETA value: 4.8e-10\n",
      "Minibatch loss at step 0: 2.305588\n",
      "Minibatch accuracy: 5.5%\n",
      "Validation accuracy: 36.187901\n",
      "Minibatch loss at step 500: 0.432577\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 85.376603\n",
      "Minibatch loss at step 1000: 0.349506\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 86.899038\n",
      "Minibatch loss at step 1500: 0.413200\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 88.381410\n",
      "Minibatch loss at step 2000: 0.231864\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 88.842147\n",
      "Minibatch loss at step 2500: 0.283091\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.413061\n",
      "Minibatch loss at step 3000: 0.206250\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.833734\n",
      "Test dataset accuracy 94.98%\n",
      "BETA value: 5.2e-10\n",
      "Minibatch loss at step 0: 2.316199\n",
      "Minibatch accuracy: 5.5%\n",
      "Validation accuracy: 41.125801\n",
      "Minibatch loss at step 500: 0.429665\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 84.915865\n",
      "Minibatch loss at step 1000: 0.326474\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 86.959135\n",
      "Minibatch loss at step 1500: 0.408389\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 88.181090\n",
      "Minibatch loss at step 2000: 0.235463\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.042468\n",
      "Minibatch loss at step 2500: 0.276175\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.312901\n",
      "Minibatch loss at step 3000: 0.213909\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 89.663462\n",
      "Test dataset accuracy 95.09%\n",
      "BETA value: 5.6e-10\n",
      "Minibatch loss at step 0: 2.318182\n",
      "Minibatch accuracy: 11.7%\n",
      "Validation accuracy: 36.598558\n",
      "Minibatch loss at step 500: 0.424470\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 85.306490\n",
      "Minibatch loss at step 1000: 0.344177\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 87.179487\n",
      "Minibatch loss at step 1500: 0.429420\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 88.050881\n",
      "Minibatch loss at step 2000: 0.245591\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 88.772035\n",
      "Minibatch loss at step 2500: 0.290206\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.413061\n",
      "Minibatch loss at step 3000: 0.207997\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.523237\n",
      "Test dataset accuracy 94.87%\n",
      "BETA value: 6e-10\n",
      "Minibatch loss at step 0: 2.338373\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 35.186298\n",
      "Minibatch loss at step 500: 0.422561\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 84.865785\n",
      "Minibatch loss at step 1000: 0.338891\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 86.929087\n",
      "Minibatch loss at step 1500: 0.438629\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 88.161058\n",
      "Minibatch loss at step 2000: 0.235941\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 88.812099\n",
      "Minibatch loss at step 2500: 0.268489\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.663462\n",
      "Minibatch loss at step 3000: 0.186278\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 89.713542\n",
      "Test dataset accuracy 94.94%\n",
      "BETA value: 6.4e-10\n",
      "Minibatch loss at step 0: 2.324202\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 42.127404\n",
      "Minibatch loss at step 500: 0.437602\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 84.815705\n",
      "Minibatch loss at step 1000: 0.344519\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 87.189503\n",
      "Minibatch loss at step 1500: 0.434176\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 88.110978\n",
      "Minibatch loss at step 2000: 0.239890\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 88.872196\n",
      "Minibatch loss at step 2500: 0.297231\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.453125\n",
      "Minibatch loss at step 3000: 0.202254\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.533253\n",
      "Test dataset accuracy 95.01%\n",
      "BETA value: 6.8e-10\n",
      "Minibatch loss at step 0: 2.310441\n",
      "Minibatch accuracy: 11.7%\n",
      "Validation accuracy: 37.199519\n",
      "Minibatch loss at step 500: 0.428930\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 85.156250\n",
      "Minibatch loss at step 1000: 0.345513\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 87.189503\n",
      "Minibatch loss at step 1500: 0.434541\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 88.131010\n",
      "Minibatch loss at step 2000: 0.241067\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 88.802083\n",
      "Minibatch loss at step 2500: 0.285316\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.232772\n",
      "Minibatch loss at step 3000: 0.222041\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.503205\n",
      "Test dataset accuracy 94.83%\n",
      "BETA value: 7.2e-10\n",
      "Minibatch loss at step 0: 2.332226\n",
      "Minibatch accuracy: 7.0%\n",
      "Validation accuracy: 35.316506\n",
      "Minibatch loss at step 500: 0.430403\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 84.995994\n",
      "Minibatch loss at step 1000: 0.346154\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.119391\n",
      "Minibatch loss at step 1500: 0.443915\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 87.930689\n",
      "Minibatch loss at step 2000: 0.250556\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.082532\n",
      "Minibatch loss at step 2500: 0.294560\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.352965\n",
      "Minibatch loss at step 3000: 0.197643\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.853766\n",
      "Test dataset accuracy 94.92%\n",
      "BETA value: 7.6e-10\n",
      "Minibatch loss at step 0: 2.339579\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 32.842548\n",
      "Minibatch loss at step 500: 0.452404\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 85.196314\n",
      "Minibatch loss at step 1000: 0.333074\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 87.019231\n",
      "Minibatch loss at step 1500: 0.413826\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 88.141026\n",
      "Minibatch loss at step 2000: 0.220695\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.012420\n",
      "Minibatch loss at step 2500: 0.292004\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.533253\n",
      "Minibatch loss at step 3000: 0.206460\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 89.473157\n",
      "Test dataset accuracy 94.95%\n",
      "BETA value: 8e-10\n",
      "Minibatch loss at step 0: 2.311934\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 34.885817\n",
      "Minibatch loss at step 500: 0.433231\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 84.935897\n",
      "Minibatch loss at step 1000: 0.336059\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 87.369792\n",
      "Minibatch loss at step 1500: 0.442044\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 87.960737\n",
      "Minibatch loss at step 2000: 0.217796\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 88.902244\n",
      "Minibatch loss at step 2500: 0.277432\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 89.362981\n",
      "Minibatch loss at step 3000: 0.200881\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 89.683494\n",
      "Test dataset accuracy 94.97%\n",
      "BETA value: 8.4e-10\n",
      "Minibatch loss at step 0: 2.303622\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 37.830529\n",
      "Minibatch loss at step 500: 0.435275\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 85.136218\n",
      "Minibatch loss at step 1000: 0.316708\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 87.159455\n",
      "Minibatch loss at step 1500: 0.425223\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 87.990785\n",
      "Minibatch loss at step 2000: 0.221444\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 88.932292\n",
      "Minibatch loss at step 2500: 0.277630\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.543269\n",
      "Minibatch loss at step 3000: 0.209016\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.783654\n",
      "Test dataset accuracy 95.06%\n",
      "BETA value: 8.8e-10\n",
      "Minibatch loss at step 0: 2.329148\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 29.767628\n",
      "Minibatch loss at step 500: 0.446962\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 85.326522\n",
      "Minibatch loss at step 1000: 0.341882\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 87.129407\n",
      "Minibatch loss at step 1500: 0.441392\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 87.900641\n",
      "Minibatch loss at step 2000: 0.232500\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.002404\n",
      "Minibatch loss at step 2500: 0.276718\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.212740\n",
      "Minibatch loss at step 3000: 0.197063\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.813702\n",
      "Test dataset accuracy 95.05%\n",
      "BETA value: 9.2e-10\n",
      "Minibatch loss at step 0: 2.316316\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 34.535256\n",
      "Minibatch loss at step 500: 0.418990\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 85.276442\n",
      "Minibatch loss at step 1000: 0.343431\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 86.989183\n",
      "Minibatch loss at step 1500: 0.450103\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 88.201122\n",
      "Minibatch loss at step 2000: 0.226413\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 88.792067\n",
      "Minibatch loss at step 2500: 0.299046\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.062500\n",
      "Minibatch loss at step 3000: 0.203014\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.713542\n",
      "Test dataset accuracy 95.12%\n",
      "BETA value: 9.6e-10\n",
      "Minibatch loss at step 0: 2.316645\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 37.169471\n",
      "Minibatch loss at step 500: 0.429896\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 85.096154\n",
      "Minibatch loss at step 1000: 0.337506\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.019231\n",
      "Minibatch loss at step 1500: 0.441943\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 88.110978\n",
      "Minibatch loss at step 2000: 0.235758\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.022436\n",
      "Minibatch loss at step 2500: 0.287255\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.473157\n",
      "Minibatch loss at step 3000: 0.215598\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 89.613381\n",
      "Test dataset accuracy 94.88%\n",
      "BETA value: 1e-09\n",
      "Minibatch loss at step 0: 2.294703\n",
      "Minibatch accuracy: 15.6%\n",
      "Validation accuracy: 41.256010\n",
      "Minibatch loss at step 500: 0.431793\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 85.076122\n",
      "Minibatch loss at step 1000: 0.311291\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 86.939103\n",
      "Minibatch loss at step 1500: 0.417836\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 87.980769\n",
      "Minibatch loss at step 2000: 0.236317\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 88.972356\n",
      "Minibatch loss at step 2500: 0.276097\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.312901\n",
      "Minibatch loss at step 3000: 0.208156\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 89.753606\n",
      "Test dataset accuracy 95.02%\n",
      "BETA value: 1.04e-09\n",
      "Minibatch loss at step 0: 2.332185\n",
      "Minibatch accuracy: 7.0%\n",
      "Validation accuracy: 33.613782\n",
      "Minibatch loss at step 500: 0.431415\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 85.346554\n",
      "Minibatch loss at step 1000: 0.334932\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 87.059295\n",
      "Minibatch loss at step 1500: 0.444212\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 88.181090\n",
      "Minibatch loss at step 2000: 0.237952\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.032452\n",
      "Minibatch loss at step 2500: 0.295584\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.342949\n",
      "Minibatch loss at step 3000: 0.208819\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.763622\n",
      "Test dataset accuracy 95.02%\n",
      "BETA value: 1.08e-09\n",
      "Minibatch loss at step 0: 2.305582\n",
      "Minibatch accuracy: 15.6%\n",
      "Validation accuracy: 38.792067\n",
      "Minibatch loss at step 500: 0.422921\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 85.096154\n",
      "Minibatch loss at step 1000: 0.342167\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 86.929087\n",
      "Minibatch loss at step 1500: 0.445876\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 88.100962\n",
      "Minibatch loss at step 2000: 0.234840\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 88.842147\n",
      "Minibatch loss at step 2500: 0.301275\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.503205\n",
      "Minibatch loss at step 3000: 0.196133\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.863782\n",
      "Test dataset accuracy 94.95%\n",
      "BETA value: 1.12e-09\n",
      "Minibatch loss at step 0: 2.306190\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 33.904247\n",
      "Minibatch loss at step 500: 0.438328\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 84.925881\n",
      "Minibatch loss at step 1000: 0.311868\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 86.899038\n",
      "Minibatch loss at step 1500: 0.439537\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 88.151042\n",
      "Minibatch loss at step 2000: 0.233994\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 88.922276\n",
      "Minibatch loss at step 2500: 0.293802\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.523237\n",
      "Minibatch loss at step 3000: 0.226363\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.573317\n",
      "Test dataset accuracy 94.99%\n",
      "BETA value: 1.16e-09\n",
      "Minibatch loss at step 0: 2.300204\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 33.884215\n",
      "Minibatch loss at step 500: 0.451439\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 85.136218\n",
      "Minibatch loss at step 1000: 0.335624\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.219551\n",
      "Minibatch loss at step 1500: 0.438562\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 87.970753\n",
      "Minibatch loss at step 2000: 0.235452\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.082532\n",
      "Minibatch loss at step 2500: 0.292497\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.182692\n",
      "Minibatch loss at step 3000: 0.208985\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 89.593349\n",
      "Test dataset accuracy 94.92%\n"
     ]
    }
   ],
   "source": [
    "predacc_scores = []\n",
    "for multiplier in range(0, 30):\n",
    "    beta = 1e-11 * (4*multiplier)\n",
    "    print('BETA value:', beta)\n",
    "    predacc_scores.append((beta, inference(beta)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fdb93d58d10>]"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAENCAYAAADgwHn9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdba84496d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot([beta for beta,_ in predacc_scores], [acc for beta, acc in predacc_scores])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8e-11, 95.170000000000002)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = [acc for beta, acc in predacc_scores]\n",
    "predacc_scores[l.index(max(l))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 0: 2.305056\n",
      "Minibatch accuracy: 11.7%\n",
      "Validation accuracy: 19.290865\n",
      "Minibatch loss at step 500: 1.129877\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy: 84.034455\n",
      "Minibatch loss at step 1000: 1.075351\n",
      "Minibatch accuracy: 55.5%\n",
      "Validation accuracy: 84.975962\n",
      "Minibatch loss at step 1500: 1.132594\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy: 87.389824\n",
      "Minibatch loss at step 2000: 1.071026\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy: 88.151042\n",
      "Minibatch loss at step 2500: 0.994586\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy: 88.741987\n",
      "Minibatch loss at step 3000: 1.087952\n",
      "Minibatch accuracy: 46.1%\n",
      "Validation accuracy: 88.321314\n",
      "Test dataset accuracy 93.97%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "93.969999999999999"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference(8e-11, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 0: 2.287638\n",
      "Minibatch accuracy: 19.5%\n",
      "Validation accuracy: 31.550481\n",
      "Minibatch loss at step 500: 0.420660\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 85.076122\n",
      "Minibatch loss at step 1000: 0.347805\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 87.329728\n",
      "Minibatch loss at step 1500: 0.423977\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 88.070913\n",
      "Minibatch loss at step 2000: 0.247268\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 88.982372\n",
      "Minibatch loss at step 2500: 0.300120\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.543269\n",
      "Minibatch loss at step 3000: 0.204729\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 89.633413\n",
      "Test dataset accuracy 94.97%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "94.969999999999999"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference(8e-11, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
