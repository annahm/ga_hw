{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.examples.tutorials.mnist import mnist\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set high-level variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS            \n",
    "flags.DEFINE_string('train_dir', 'data', 'Directory to put the training data.')\n",
    "flags.DEFINE_boolean('fake_data', False, 'If true, uses fake data for unit testing.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.platform.default._flags._FlagValues'>\n"
     ]
    }
   ],
   "source": [
    "print(type(FLAGS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=128\n",
    "IMAGE_PIXELS=28*28\n",
    "hidden1_units=1024\n",
    "hidden2_units=1024\n",
    "NUM_CLASSES=10\n",
    "learning_rate = 0.5\n",
    "max_steps = 3001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extract data in needed formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "data_sets = input_data.read_data_sets(FLAGS.train_dir, FLAGS.fake_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sets.train.labels[138]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training set', (55000, 784), (55000, 10), (55000,))\n",
      "('Validation set', (5000, 784), (5000, 10), (5000,))\n",
      "('Test set', (10000, 784), (10000, 10), (10000,))\n"
     ]
    }
   ],
   "source": [
    "def reformat_label(labels):\n",
    "  # dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "  labels = (np.arange(NUM_CLASSES) == labels[:,None]).astype(np.float32)\n",
    "  return labels\n",
    "\n",
    "# Datasets are Nx724\n",
    "train_dataset = data_sets.train.images\n",
    "valid_dataset = data_sets.validation.images\n",
    "test_dataset = data_sets.test.images\n",
    "\n",
    "# Labels are (N,10) (1hot encoded)\n",
    "train_labels = reformat_label(data_sets.train.labels)\n",
    "valid_labels = reformat_label(data_sets.validation.labels)\n",
    "test_labels = reformat_label(data_sets.test.labels)\n",
    "\n",
    "# Same labels are (N,) (NOT 1hot encoded)\n",
    "train_labels_int = data_sets.train.labels\n",
    "valid_labels_int = data_sets.validation.labels\n",
    "test_labels_int = data_sets.test.labels\n",
    "\n",
    "print('Training set', train_dataset.shape, train_labels.shape, train_labels_int.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape, valid_labels_int.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape, test_labels_int.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build the Graph\n",
    "Questions:  \n",
    "* ** eval_correct: ** Set up eval_correct tensor to call in order to periodically evalulate againstt my validation set.  What gets execcuted in the graph when I do this?  Is it just simply comparing the calculated logits against the responses?  Or is it now using the validated set to continue training the graph?\n",
    "* ** tensor states: ** How to peer into each tensor to view it's values?  How to construct breakpoints in the midst of processing to look at individual tensors.\n",
    "* ** train_prediction: ** I created this when running the training data thru the loop and then calculating the prediction based on weights.  However if I run my validation sets thru this, am I calculating a train_prediction as well with the validation set?  How does the graph know not to adjust the weights for this round?\n",
    "* **placeholder_labels and placeholder_labels_int: ** kludge-y\n",
    "* **eval section:** When I use these tensors to check accuracy across entire test set, I noticed that it does not run the logits through the softmax.  Shouldn't it?  It seems that the predictions should always go thru the Softmax?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graphn = tf.Graph()\n",
    "\n",
    "with graphn.as_default():\n",
    "    # Inputs and PLACEHOLDERS\n",
    "    # Originally set the shape for each of these to specifically use batch_size for # rows\n",
    "    # This caused problems when I wanted to run the entire test set to get an accuracy score\n",
    "    # because batch_size was 128 and the test set dimension was much larger (10000, 784)\n",
    "    # then I saw that the tutorial used None instead and that worked for everything\n",
    "    images_placeholder     =tf.placeholder(tf.float32, shape=(None, IMAGE_PIXELS))\n",
    "    labels_placeholder     =tf.placeholder(tf.float32, shape=(None, NUM_CLASSES))\n",
    "    labels_placeholder_int =tf.placeholder(tf.int32, shape=(None))  \n",
    "   \n",
    "    # INFERENCE\n",
    "    with tf.name_scope('hidden1') as scope:\n",
    "        weights = tf.Variable(tf.truncated_normal([IMAGE_PIXELS, hidden1_units], \n",
    "            stddev=1.0 / math.sqrt(float(IMAGE_PIXELS))), name='weights')\n",
    "        biases = tf.Variable(tf.zeros([hidden1_units]), name = 'biases')\n",
    "        hidden1 = tf.nn.relu(tf.matmul(images_placeholder, weights) + biases)\n",
    "\n",
    "    with tf.name_scope('hidden2') as scope:\n",
    "        weights = tf.Variable(tf.truncated_normal([hidden1_units, hidden2_units],\n",
    "            stddev=1.0 / math.sqrt(float(hidden1_units))), name='weights')\n",
    "        biases = tf.Variable(tf.zeros([hidden2_units]), name='biases')\n",
    "        hidden2 = tf.nn.relu(tf.matmul(hidden1, weights) + biases)\n",
    "\n",
    "    with tf.name_scope('softmax_linear') as scope:\n",
    "        weights = tf.Variable(tf.truncated_normal([hidden2_units, NUM_CLASSES],\n",
    "            stddev=1.0 / math.sqrt(float(hidden2_units))),name='weights')\n",
    "        biases = tf.Variable(tf.zeros([NUM_CLASSES]),name='biases')\n",
    "        logits = tf.matmul(hidden2, weights) + biases\n",
    "      \n",
    "    # LOSS\n",
    "    # softmax_cross_entropy needs labels_placeholder to be float32 1hot encoded\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, labels_placeholder))\n",
    "    \n",
    "    # TRAINING\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    # EVAL (use either eval_acc op or the eval_correct op)\n",
    "    # Gets the index of the largest value across the tensor, test_a is prediction, test_b is response\n",
    "    test_a = tf.argmax(logits,1)\n",
    "    test_b = tf.argmax(labels_placeholder,1)\n",
    "    # Boolean tensor\n",
    "    correct_prediction = tf.equal(test_a, test_b)\n",
    "    cast_float = tf.cast(correct_prediction, tf.float32)\n",
    "    # Get mean across tensor, ie sum/length\n",
    "    eval_acc = tf.reduce_mean(cast_float)\n",
    "    # in_top_k requires labels_placeholder to be int32 or int64 list\n",
    "    correct = tf.nn.in_top_k(logits, labels_placeholder_int, 1)\n",
    "    eval_correct = tf.reduce_sum(tf.cast(correct, tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<bound method Tensor.get_shape of <tf.Tensor 'Placeholder_2:0' shape=(128,) dtype=int32>>, <bound method Tensor.get_shape of <tf.Tensor 'Placeholder_1:0' shape=(128, 10) dtype=float32>>)\n"
     ]
    }
   ],
   "source": [
    "print(labels_placeholder_int.get_shape, labels_placeholder.get_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training Loop\n",
    "Questions:\n",
    "* **feed_dict:** See a) below. What does it mean if I don't include one of the placeholders in the graph?  Seems like that tensor just does not get called?  If I call that tensor, I have to includ the right placeholder or it chokes\n",
    "* ** Mini batches:** When it is appropriate to do mini-batches?  Is that just for training?  What if I just want to simply predict (and evaluate)?  Why do I need to do this as a mini-batch?\n",
    "* **train vs. test**: How does the graph know when you are training it to update weights, vs just feeding it a data set to calculate accuracy (and not change weights or bias values)?  Would you ever want to push the test data into smaller batch sizes?  In the exmaple below, I did not do that... Is it that if you don't call the train_op vector which tries to minimize loss which would then update the weights and biases?  But when I removed that from my final test run, I did not notice any difference in errors from when I did call for loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 0: 2.299736\n",
      "Minibatch accuracy: 14.1%\n",
      "Validation accuracy: 26.970167\n",
      "Minibatch loss at step 500: 0.153297\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 96.803245\n",
      "Minibatch loss at step 1000: 0.019522\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 97.597866\n",
      "Minibatch loss at step 1500: 0.054390\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 97.580363\n",
      "Minibatch loss at step 2000: 0.054306\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 97.302871\n",
      "Minibatch loss at step 2500: 0.009856\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 98.086604\n",
      "Minibatch loss at step 3000: 0.002164\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 98.289689\n",
      "Test dataset accuracy 0.01%\n"
     ]
    }
   ],
   "source": [
    "# TRAINING LOOP\n",
    "\n",
    "with tf.Session(graph=graphn) as session:\n",
    "    init = tf.initialize_all_variables().run()\n",
    "    sess = tf.Session()\n",
    "    \n",
    "    for step in xrange(max_steps):\n",
    "        # Generates the starting index from train_dataset to extract the minibatch\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        batch_labels_int = train_labels_int[offset:(offset + batch_size)]\n",
    "        # A) The fact that I only feed it labels_placeholder and not labels_placeholder_int?\n",
    "        feed_dict = {images_placeholder : batch_data, labels_placeholder : batch_labels, labels_placeholder_int : batch_labels_int}\n",
    "        _, l, predictions, num_correct = session.run([train_op, loss, train_prediction, eval_correct], feed_dict=feed_dict)       \n",
    "        \n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            # Evaluate the entire dataset in specified batch-size increments, return all predictions\n",
    "            acc, eval_predictions = do_eval(valid_dataset, valid_labels, valid_labels_int, batch_size, \n",
    "                    images_placeholder, labels_placeholder, labels_placeholder_int)\n",
    "            print(\"Validation accuracy: %4.6f\" %(acc))\n",
    "    \n",
    "    fdict = {images_placeholder : test_dataset, \n",
    "             labels_placeholder : test_labels,\n",
    "             labels_placeholder_int : test_labels_int}\n",
    "    logits, eval_correct, test_acc = session.run([logits, eval_correct, eval_acc], feed_dict=fdict)\n",
    "    print('Test dataset accuracy %4.2f%%' %(100*eval_correct/float(len(test_dataset))))\n",
    "    \n",
    "    #print(sess.run(logits, feed_dict={images_placeholder: test_dataset,labels_placeholder: test_labels}))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.], dtype=float32)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def do_eval(valid_dataset, valid_labels, valid_labels_int, batch_size, \n",
    "            images_placeholder, labels_placeholder, labels_placeholder_int):\n",
    "    true_count = 0  # Counts the number of correct predictions.\n",
    "    steps_per_epoch = valid_dataset.shape[0] // batch_size\n",
    "    num_examples = steps_per_epoch * batch_size\n",
    "    eval_predictions = []\n",
    "    for step in xrange(steps_per_epoch):\n",
    "        eval_offset = (step * batch_size) % (valid_labels.shape[0] - batch_size)\n",
    "        eval_batch_data = valid_dataset[eval_offset:(eval_offset + batch_size), :]\n",
    "        eval_batch_labels = valid_labels[eval_offset:(eval_offset + batch_size), :]\n",
    "        eval_batch_labels_int = valid_labels_int[eval_offset:(eval_offset + batch_size)]\n",
    "        f_dict = {images_placeholder : eval_batch_data, labels_placeholder : eval_batch_labels,\n",
    "                  labels_placeholder_int : eval_batch_labels_int}\n",
    "        eval_predictions += session.run([eval_correct, loss, train_op], feed_dict=f_dict)\n",
    "    #print(\"validation accuracy: %4.6f%%\" %(100*sum(eval_predictions)/float(num_examples)))    \n",
    "    return 100*sum(eval_predictions)/float(num_examples), eval_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels_placeholder_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
